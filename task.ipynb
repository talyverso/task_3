{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "task1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "S2C8-87sHTIu",
        "DEgodl3_HTJB",
        "5wfYznveHTJu",
        "dihVpXfpHTJ0",
        "0tnNQ2GiHTKF"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xARKIlSDlW7b",
        "colab_type": "text"
      },
      "source": [
        "### **Amazon product review analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jM3f3r_lj8M",
        "colab_type": "text"
      },
      "source": [
        "To compare three diffferent models, I've decided to use the ROC AUC score as the target metric. This metric usually used when all classes have the same value for the research, so this is the case of this work. Moreover, the classes in the sample dataset are balanced, so ir is possible to use this metric. The FastText, TFIDF+XGBoost and RNN model were built.\n",
        "\n",
        "\n",
        "Firstly, the import of the all necessary packages and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Cr89wMHkG_",
        "colab_type": "code",
        "outputId": "2479340c-db7c-4e11-a430-ba24f51f559c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.3)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3016314 sha256=c3bf1fb9fbee919d9418377be8eb20a69143e86ee6a0042dc20ddd7aa2a0ecd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MESKCq8MHSuN",
        "colab_type": "code",
        "outputId": "85cdb822-3d41-4ed8-e0ea-0c5c144e2efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import fasttext\n",
        "import string\n",
        "import re\n",
        "import bz2\n",
        "import csv\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K8Ch61iHnWF",
        "colab_type": "code",
        "outputId": "81587650-7c1f-4215-c9af-fd7aa14e56bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loading of the training data \n",
        "train = bz2.BZ2File(\"train.ft.txt.bz2\")\n",
        "train = train.readlines()\n",
        "train = [x.decode('utf-8') for x in train]\n",
        "print(len(train)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3600000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKyoB74rH8qk",
        "colab_type": "code",
        "outputId": "e4c4e907-82a8-460a-e011-a7ed3b14cf45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Loading of the test data \n",
        "test = bz2.BZ2File(\"test.ft.txt.bz2\")\n",
        "test = test.readlines()\n",
        "test = [x.decode('utf-8') for x in test]\n",
        "print(len(test), 'number of records in the test set') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000 number of records in the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFTZJYMTH8mO",
        "colab_type": "code",
        "outputId": "d787e5af-0481-403b-9358-43ad74d013b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "train[1:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n\",\n",
              " '__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n',\n",
              " \"__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\\n\",\n",
              " \"__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\\n\",\n",
              " \"__label__2 an absolute masterpiece: I am quite sure any of you actually taking the time to read this have played the game at least once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsuda's music contributed greatly to the mood of every single minute of the whole game.Composed of 3 CDs and quite a few songs (I haven't an exact count), all of which are heart-rendering and impressively remarkable, this soundtrack is one I assure you you will not forget. It has everything for every listener -- from fast-paced and energetic (Dancing the Tokage or Termina Home), to slower and more haunting (Dragon God), to purely beautifully composed (Time's Scar), to even some fantastic vocals (Radical Dreamers).This is one of the best videogame soundtracks out there, and surely Mitsuda's best ever. ^_^\\n\",\n",
              " '__label__1 Buyer beware: This is a self-published book, and if you want to know why--read a few paragraphs! Those 5 star reviews must have been written by Ms. Haddon\\'s family and friends--or perhaps, by herself! I can\\'t imagine anyone reading the whole thing--I spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another. It is most definitely bad enough to be entered into some kind of a \"worst book\" contest. I can\\'t believe Amazon even sells this kind of thing. Maybe I can offer them my 8th grade term paper on \"To Kill a Mockingbird\"--a book I am quite sure Ms. Haddon never heard of. Anyway, unless you are in a mood to send a book to someone as a joke---stay far, far away from this one!\\n',\n",
              " '__label__2 Glorious story: I loved Whisper of the wicked saints. The story was amazing and I was pleasantly surprised at the changes in the book. I am not normaly someone who is into romance novels, but the world was raving about this book and so I bought it. I loved it !! This is a brilliant story because it is so true. This book was so wonderful that I have told all of my friends to read it. It is not a typical romance, it is so much more. Not reading this book is a crime, becuase you are missing out on a heart warming story.\\n',\n",
              " \"__label__2 A FIVE STAR BOOK: I just finished reading Whisper of the Wicked saints. I fell in love with the caracters. I expected an average romance read, but instead I found one of my favorite books of all time. Just when I thought I could predict the outcome I was shocked ! The writting was so descriptive that my heart broke when Julia's did and I felt as if I was there with them instead of just a distant reader. If you are a lover of romance novels then this is a must read. Don't let the cover fool you this book is spectacular!\\n\",\n",
              " '__label__2 Whispers of the Wicked Saints: This was a easy to read book that made me want to keep reading on and on, not easy to put down.It left me wanting to read the follow on, which I hope is coming soon. I used to read a lot but have gotten away from it. This book made me want to read again. Very enjoyable.\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ArUFcSH8Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.DataFrame(train)\n",
        "train.to_csv(\"train.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "test = pd.DataFrame(test)\n",
        "test.to_csv(\"test.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2k-8LqglQQX",
        "colab_type": "text"
      },
      "source": [
        "## **The FastText model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1967M6fH7sY",
        "colab_type": "code",
        "outputId": "559b2eeb-b4c3-47ef-ecce-52ad0460ebf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = fasttext.train_supervised('train.txt',label_prefix='__label__', thread=4, epoch = 50)\n",
        "print(model.labels, 'are the labels or targets the model is predicting')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__label__1', '__label__2'] are the labels or targets the model is predicting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQTc7LckH7pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing the __label__1 and __label__2 from the testset to run the predict function \n",
        "test_n = [w.replace('__label__2 ', '') for w in test]\n",
        "test_n = [w.replace('__label__1 ', '') for w in test_n]\n",
        "test_n = [w.replace('\\n', '') for w in test_n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_roszwLH7jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(test_n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7io3E69H7YL",
        "colab_type": "code",
        "outputId": "c0358925-9f93-4969-f3fe-868a0db7d16e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Lets recode the actual targets to 1's and 0's from both the test set and the actual predictions  \n",
        "labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test]\n",
        "pred_labels = [0 if x == ['__label__1'] else 1 for x in pred[0]]\n",
        "\n",
        "# run the accuracy measure. \n",
        "print(roc_auc_score(labels, pred_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8954399999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NpK87NdlVvG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The fasttext model has the 89,5% ROC-AUC score, which is relatively high. Firstly, I've tried to build this model with only 10 epochs, and it gave even higher score - around 91,7%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82Pyj1UHSxR",
        "colab_type": "text"
      },
      "source": [
        "## **TFIDF + XGBoost model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Z7jwnR-tjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('train.txt', 'r', encoding ='utf-8')\n",
        "X_train = []\n",
        "for i in f: \n",
        "    X_train.append(i[11:])\n",
        "f.close()\n",
        "\n",
        "f = open('test.txt', 'r', encoding ='utf-8')\n",
        "X_test = []\n",
        "for i in f: \n",
        "    X_test.append(i[11:])\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAfqmHl3-tsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('train.txt', 'r', encoding ='utf-8')\n",
        "y_train = []\n",
        "for i in f: \n",
        "    if i[:10] == '__label__1':\n",
        "        y_train.append(0)\n",
        "    else:\n",
        "        y_train.append(1)\n",
        "f.close()\n",
        "\n",
        "f = open('test.txt', 'r', encoding ='utf-8')\n",
        "y_test = []\n",
        "for i in f: \n",
        "    if i[:10] == '__label__1':\n",
        "        y_test.append(0)\n",
        "    else:\n",
        "        y_test.append(1)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6nJ9Hx9HSxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaeoBkH-HSxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Token (str_input):\n",
        "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
        "    porter_stemmer=nltk.PorterStemmer()\n",
        "    words = [porter_stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLASh9A7HSxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=Token, stop_words=stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JerRwdyKHSxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainXGB = vectorizer.fit_transform(X_train)\n",
        "testXGB = vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd_uQtJxHSxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB = XGBClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cvXbC-kHSxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB.fit(trainXGB, y_train)\n",
        "\n",
        "XGB.save_model(\"XGB.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fv9OvTHSxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB.load_model(\"XGB.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P7CoDRDHSyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB_pred = XGB_model.predict(test)\n",
        "\n",
        "with open('XGB_pred', 'wb') as f:\n",
        "     pickle.dump(XGB_pred, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7wNNTA2HSyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('XGB_pred', 'rb') as f:\n",
        "     XGB_pred = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeB9muGeHSym",
        "colab_type": "code",
        "outputId": "a538fd60-a5da-44d0-ffc6-01d68e922202",
        "colab": {}
      },
      "source": [
        "XGB_auc = roc_auc_score(y_test, XGB_pred)\n",
        "print (\"SCORE:\", XGB_auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORE: 0.85914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3mHh0Z1oz-Q",
        "colab_type": "text"
      },
      "source": [
        "The result of ML model is pretty much lower, than the performance of FastText model (85,9 vs 91,7%). Let's have a look at the RNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jev2IVlupFdR",
        "colab_type": "text"
      },
      "source": [
        "## **RNN model**\n",
        "\n",
        "I've decided to build the RNN model with LSTM layer, 2 Dense layers and Dropout layer for avoiding the overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN9BvZlVMM9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX6edA-g_7ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token = Tokenizer(num_words=500)\n",
        "token.fit_on_texts(X_train)\n",
        "sequences = token.texts_to_sequences(X_train)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf-RvKkA_7r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RNN():\n",
        "    inputs = Input(name='inputs',shape=[100])\n",
        "    layer = Embedding(500,50,input_length=100)(inputs) # Embedding layer\n",
        "    layer = LSTM(64)(layer) # Recurrent layer\n",
        "    layer = Dense(256, activation='relu')(layer) # Fully connected layer\n",
        "    layer = Dropout(0.5)(layer) # Dropout for regularization\n",
        "    layer = Dense(1,name='out_layer', activation='relu')(layer) # Fully connected layer\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_9JV3z8_8Xm",
        "colab_type": "code",
        "outputId": "091f274d-7332-4be7-cf3e-1e27319e77ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = RNN()\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inputs (InputLayer)          [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 100, 50)           25000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                29440     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "out_layer (Dense)            (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 71,337\n",
            "Trainable params: 71,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgvvvNsiFIpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg0uGTNrMVqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the data to RNN with batch size of 256 and 3 epochs\n",
        "model.fit(sequences_matrix,y_train,batch_size=256,epochs=3,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5a4Sh_W_8Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roc_auc_score(y_test, model.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBotbC__ql3W",
        "colab_type": "text"
      },
      "source": [
        "Unfortunately, I couldn't manage to fit this model even with only 3 epochs. My Colab just restarted runtime each time that I've started to run cells. I can assume, that this model will provide better results, than the previous 2, especially after some ajustments, but I cannot prove it due to the limits of computing power. \n",
        "\n",
        "Among the FastText and the TFIDF + XGBoost model the first one shows better results. "
      ]
    }
  ]
}
